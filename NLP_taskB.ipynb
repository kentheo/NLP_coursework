{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_taskB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ykpGgBeqJNwh",
        "colab_type": "code",
        "outputId": "2babfe5c-cb95-4e99-f401-e94bd0b94f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from keras.preprocessing import text, sequence\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q7chG5g8RmXb"
      },
      "cell_type": "markdown",
      "source": [
        "### Device Selection"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zi6L64YuQsLS",
        "outputId": "01041428-2b20-47bb-df94-ca124511f1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "vkLG6yuPFtiC",
        "colab_type": "code",
        "outputId": "09483575-68da-483a-a70f-ac1947b08b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# We set a random seed to ensure that your results are reproducible.\n",
        "if USE_CUDA:\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f41a2a601d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "g72boCXeJWru",
        "colab_type": "code",
        "outputId": "3b0fda3c-4162-4a65-f431-edca7fc23d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1499
        }
      },
      "cell_type": "code",
      "source": [
        "# Download Training Data\n",
        "!wget https://competitions.codalab.org/my/datasets/download/60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
        "!unzip 60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
        "!unzip training-v1.zip\n",
        "!unzip trial-data.zip\n",
        "!rm '60e40c68-a85d-4320-bef1-d2fe26bb45ca'\n",
        "!rm training-v1.zip\n",
        "!rm trial-data.zip\n",
        "!rm -r __MACOSX\n",
        "\n",
        "# Download Test Data\n",
        "!wget https://competitions.codalab.org/my/datasets/download/5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "!unzip 5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "!rm 5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "\n",
        "!wget https://competitions.codalab.org/my/datasets/download/bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "!unzip bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "!rm bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "\n",
        "!wget https://competitions.codalab.org/my/datasets/download/38273e56-2ab0-4773-82bf-95aec51bba69\n",
        "!unzip 38273e56-2ab0-4773-82bf-95aec51bba69\n",
        "!rm 38273e56-2ab0-4773-82bf-95aec51bba69"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-01 16:32:01--  https://competitions.codalab.org/my/datasets/download/60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/787f6/start-kit.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=5cb33a47d3be82036425955e971776bff04eb2d600eb4a2f02940396943271ee&X-Amz-Date=20190301T163157Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 16:32:02--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/787f6/start-kit.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=5cb33a47d3be82036425955e971776bff04eb2d600eb4a2f02940396943271ee&X-Amz-Date=20190301T163157Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 796236 (778K) [application/zip]\n",
            "Saving to: ‘60e40c68-a85d-4320-bef1-d2fe26bb45ca’\n",
            "\n",
            "60e40c68-a85d-4320- 100%[===================>] 777.57K   501KB/s    in 1.6s    \n",
            "\n",
            "2019-03-01 16:32:05 (501 KB/s) - ‘60e40c68-a85d-4320-bef1-d2fe26bb45ca’ saved [796236/796236]\n",
            "\n",
            "Archive:  60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
            " extracting: training-v1.zip         \n",
            "  inflating: trial-data.zip          \n",
            "Archive:  training-v1.zip\n",
            "  inflating: offenseval-training-v1.tsv  \n",
            "  inflating: readme-trainingset-v1.txt  \n",
            "  inflating: offenseval-annotation.txt  \n",
            "Archive:  trial-data.zip\n",
            "  inflating: OffensEval-READMEv1.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._OffensEval-READMEv1.txt  \n",
            "  inflating: offenseval-trial.txt    \n",
            "  inflating: __MACOSX/._offenseval-trial.txt  \n",
            "--2019-03-01 16:32:21--  https://competitions.codalab.org/my/datasets/download/5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/7b71f/Test%20A%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=8b0188e45d469f2f6673d4e385280aabd3dc3fe88861bb961bebeae892d9bf4a&X-Amz-Date=20190301T163217Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 16:32:22--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/7b71f/Test%20A%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=8b0188e45d469f2f6673d4e385280aabd3dc3fe88861bb961bebeae892d9bf4a&X-Amz-Date=20190301T163217Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63867 (62K) [application/zip]\n",
            "Saving to: ‘5cac0f56-bb6d-40fa-8041-caf8aa13d09d’\n",
            "\n",
            "5cac0f56-bb6d-40fa- 100%[===================>]  62.37K   121KB/s    in 0.5s    \n",
            "\n",
            "2019-03-01 16:32:23 (121 KB/s) - ‘5cac0f56-bb6d-40fa-8041-caf8aa13d09d’ saved [63867/63867]\n",
            "\n",
            "Archive:  5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
            "  inflating: testset-taska.tsv       \n",
            "  inflating: readme-testsetA-v1.txt  \n",
            "--2019-03-01 16:32:30--  https://competitions.codalab.org/my/datasets/download/bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4c1e9/Test%20B%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e26418bc120e9bbd30f96d436ca30298a89f41bbed77a31111417205bf488d54&X-Amz-Date=20190301T163227Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 16:32:31--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4c1e9/Test%20B%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e26418bc120e9bbd30f96d436ca30298a89f41bbed77a31111417205bf488d54&X-Amz-Date=20190301T163227Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18670 (18K) [application/zip]\n",
            "Saving to: ‘bb373027-c8b7-48ab-9729-b1ab3fb51c17’\n",
            "\n",
            "bb373027-c8b7-48ab- 100%[===================>]  18.23K  70.7KB/s    in 0.3s    \n",
            "\n",
            "2019-03-01 16:32:32 (70.7 KB/s) - ‘bb373027-c8b7-48ab-9729-b1ab3fb51c17’ saved [18670/18670]\n",
            "\n",
            "Archive:  bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
            "  inflating: readme-testsetB-v1.txt  \n",
            "  inflating: testset-taskb.tsv       \n",
            "--2019-03-01 16:32:38--  https://competitions.codalab.org/my/datasets/download/38273e56-2ab0-4773-82bf-95aec51bba69\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4003e/Test%20C%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b3b47ec9086237ae3547883c54c17770860758e2f7e99478cbcf831f30fd0f80&X-Amz-Date=20190301T163235Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 16:32:40--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4003e/Test%20C%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b3b47ec9086237ae3547883c54c17770860758e2f7e99478cbcf831f30fd0f80&X-Amz-Date=20190301T163235Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17439 (17K) [application/zip]\n",
            "Saving to: ‘38273e56-2ab0-4773-82bf-95aec51bba69’\n",
            "\n",
            "38273e56-2ab0-4773- 100%[===================>]  17.03K  66.0KB/s    in 0.3s    \n",
            "\n",
            "2019-03-01 16:32:41 (66.0 KB/s) - ‘38273e56-2ab0-4773-82bf-95aec51bba69’ saved [17439/17439]\n",
            "\n",
            "Archive:  38273e56-2ab0-4773-82bf-95aec51bba69\n",
            "  inflating: test_set_taskc.tsv      \n",
            "  inflating: readme-testsetc-v1.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3kem2OltJNwk",
        "colab_type": "code",
        "outputId": "b4c6bb3f-c631-4e6f-fc6d-2566e2479c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Path files for training and test data\n",
        "train_path = 'offenseval-training-v1.tsv'\n",
        "test_task_a_path = 'testset_taska.tsv'\n",
        "test_task_b_path = 'testset_taskb.tsv'\n",
        "test_task_c_path = 'test_set_taskc.tsv'\n",
        "\n",
        "tweets = []\n",
        "subtask_a, subtask_b, subtask_c = [], [], []\n",
        "\n",
        "with open(train_path) as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    print(reader.fieldnames)\n",
        "    for row in reader:\n",
        "        tweets.append(row['tweet'])\n",
        "        subtask_a.append(row['subtask_a'])\n",
        "        subtask_b.append(row['subtask_b'])\n",
        "        subtask_c.append(row['subtask_c'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M_hlffDfJNwo",
        "colab_type": "code",
        "outputId": "573849e2-9a08-42a2-feec-7f33915353c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print(tweets[0])\n",
        "print(subtask_a[0])\n",
        "print(subtask_b[0])\n",
        "print(subtask_c[0])\n",
        "print(\"Number of tweets:\", len(tweets))\n",
        "print(\"Number of labels:\", len(subtask_a))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@USER She should ask a few native Americans what their take on this is.\n",
            "OFF\n",
            "UNT\n",
            "NULL\n",
            "Number of tweets: 13240\n",
            "Number of labels: 13240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AW8QWcUnsycB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(list_texts):\n",
        "    \n",
        "    output = []\n",
        "    for text in list_texts:\n",
        "        ## Remove puncuation\n",
        "        text = text.translate(string.punctuation)\n",
        "\n",
        "        ## Convert words to lower case and split them\n",
        "        text = text.lower().split()\n",
        "\n",
        "        ## Remove stop words\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "\n",
        "        text = \" \".join(text)\n",
        "        ## Clean the text\n",
        "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "        text = re.sub(r\"what's\", \"what is \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"i'm\", \"i am \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "        text = re.sub(r\",\", \" \", text)\n",
        "        text = re.sub(r\"\\.\", \" \", text)\n",
        "        text = re.sub(r\"!\", \" ! \", text)\n",
        "        text = re.sub(r\"\\/\", \" \", text)\n",
        "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "        text = re.sub(r\"\\+\", \" + \", text)\n",
        "        text = re.sub(r\"\\-\", \" - \", text)\n",
        "        text = re.sub(r\"\\=\", \" = \", text)\n",
        "        text = re.sub(r\"'\", \" \", text)\n",
        "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "        text = re.sub(r\":\", \" : \", text)\n",
        "        text = re.sub(r\" e g \", \" eg \", text)\n",
        "        text = re.sub(r\" b g \", \" bg \", text)\n",
        "        text = re.sub(r\" u s \", \" american \", text)\n",
        "        text = re.sub(r\"\\0s\", \"0\", text)\n",
        "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "        text = re.sub(r\"e - mail\", \"email\", text)\n",
        "        text = re.sub(r\"j k\", \"jk\", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "        ## Stemming\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "        output.append(text)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxFLYys0MegL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tokenized_corpus(corpus):\n",
        "\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoQqdbOcMgcV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the placeholder token\n",
        "  word2idx['<pad>'] = 0\n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbH4PMDRMj3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
        "\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "\n",
        "  data_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  \n",
        "  data_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # we fill it with our vectorized sentences \n",
        "  \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, data_lengths)):\n",
        "    data_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  label_tensor = torch.Tensor(labels)\n",
        "  \n",
        "  return data_tensor, label_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Kqx4W-TIVKH",
        "colab_type": "code",
        "outputId": "763b2066-84ef-4748-f064-aea22ce734e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "train = preprocess_text(tweets)\n",
        "\n",
        "train_labels = np.array(subtask_b)\n",
        "train_labels[train_labels == 'UNT'] = 0.\n",
        "train_labels[train_labels == 'TIN'] = 1.\n",
        "\n",
        "nulls_idx = [i for i, x in enumerate(train_labels) if x == \"NULL\"]\n",
        "print(\"nulls indexes:\", len(nulls_idx))\n",
        "\n",
        "# Remove all NULLs from Training set\n",
        "for index in sorted(nulls_idx, reverse=True):\n",
        "    del train[index]\n",
        "\n",
        "# Remove all NULLs from Training labels\n",
        "train_labels = train_labels[train_labels != 'NULL']\n",
        "\n",
        "# Turn the array of strings to floats 0s and 1s\n",
        "train_labels = train_labels.astype(np.float)\n",
        "\n",
        "print(\"Number of TIN labels: \", train_labels.sum())\n",
        "print(\"Length of train set: {}, Length of train labels: {}\".format(len(train), len(train_labels)))\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train)\n",
        "print(\"Tokenized corpus size:\", len(tokenized_corpus))\n",
        "\n",
        "sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "max_len = np.max(np.array(sent_lengths))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nulls indexes: 8840\n",
            "Number of TIN labels:  3876.0\n",
            "Length of train set: 4400, Length of train labels: 4400\n",
            "Tokenized corpus size: 4400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YBlNmMrbtCv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_train_test_dataset(X, y, test_split=0.1, shuffle_dataset=True, random_seed=None):\n",
        "    indices = list(range(X.shape[0]))\n",
        "    test_split_idx = int(np.floor(test_split * X.shape[0]))\n",
        "\n",
        "    if shuffle_dataset and random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    elif shuffle_dataset:\n",
        "        np.random.seed()\n",
        "        np.random.shuffle(indices)\n",
        "    train_indices, test_indices = indices[test_split_idx:], indices[:test_split_idx]\n",
        "\n",
        "    X_train_ = X[train_indices]\n",
        "    X_test_ = X[test_indices]\n",
        "    y_train_ = y[train_indices]\n",
        "    y_test_ = y[test_indices]\n",
        "\n",
        "    return X_train_, y_train_, X_test_, y_test_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LG_Wh84cHrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchify(data, batch_size):\n",
        "    '''\n",
        "    This function will divide data into batches\n",
        "    '''\n",
        "    \n",
        "    nbatch = data.size(0) // batch_size # divide the dataset into batches parts.\n",
        "\n",
        "    data = data.narrow(0, 0, nbatch * batch_size) # extra elments that dont fit the batches\n",
        "    \n",
        "    data = data.view(batch_size, -1).contiguous() # Evenly divide into all batches\n",
        "   \n",
        "    if USE_CUDA:\n",
        "        data = data.cuda()\n",
        "        \n",
        "    return data\n",
        "  \n",
        "\n",
        "def getBatch(data, labels, seq_length, batch_size):\n",
        "    '''\n",
        "    This function gives us batches for training\n",
        "    * this is language modelling, our targets will be the next words\n",
        "    * this function gives partial sequence samples\n",
        "    '''\n",
        "    for i in range(0, data.shape[1] - seq_length, seq_length):\n",
        "      inputs = data[:, i: i + seq_length]\n",
        "      targets = labels[i: i + batch_size]\n",
        "      yield (inputs, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0TcRISJOIlD",
        "colab_type": "code",
        "outputId": "5506b382-d2f3-412f-b612-721e434ebba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "print(\"Word2Idx size:\", len(word2idx))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Idx size: 7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xSOdAKZMlLKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_metrics(real, preds):\n",
        "    acc = accuracy_score(real, preds)\n",
        "    recall = recall_score(real, preds)\n",
        "    precision = precision_score(real, preds)\n",
        "    f1 = f1_score(real, preds, average='macro')\n",
        "    \n",
        "    return acc, recall, precision, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cp_hTxc_FHz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Create Models**"
      ]
    },
    {
      "metadata": {
        "id": "op8fkq0gUwND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder_lstm(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(Encoder_lstm, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(embed_size, 128, 3)\n",
        "        self.conv2 = nn.Conv1d(128, 256, 3)\n",
        "        self.conv3 = nn.Conv1d(256, 512, 3)\n",
        "        \n",
        "        self.lstm = nn.LSTM(75, self.hidden_size, self.num_layers)\n",
        "        \n",
        "#         self.deconv1 = nn.ConvTranspose1d(512, 256, 3, stride=2)\n",
        "#         self.deconv2 = nn.ConvTranspose1d(256, 128, 5, stride=3, padding=1)\n",
        "#         self.deconv3 = nn.ConvTranspose1d(128, 64, 6, stride=2, padding=1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(512, 1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        \n",
        "        x, h = self.lstm(x, h)\n",
        "        \n",
        "#         x = self.relu(self.deconv1(x))\n",
        "#         x = self.relu(self.deconv2(x))\n",
        "#         x = self.relu(self.deconv3(x))\n",
        "        # Take the average of the embeddings\n",
        "        x = x.mean(2)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        return x, h\n",
        "  \n",
        "    def init_hidden(self,batch_size):\n",
        "        # Hidden layer initialized to 0\n",
        "        return (torch.zeros(self.num_layers, 512, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, 512, self.hidden_size))\n",
        "  \n",
        "class Model_lstm_conv(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(Model_lstm_conv, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, self.embed_size)\n",
        "        self.dropout = nn.Dropout2d(0.5)\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.embed_size, self.hidden_size, self.num_layers)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(self.embed_size, 30, 3)\n",
        "        self.conv2 = nn.Conv1d(30, 5, 3)\n",
        "        self.fc1 = nn.Linear(5, 1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x, h = self.lstm(x, h)\n",
        "        \n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "        x = x.mean(2)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        \n",
        "        return x, h\n",
        "\n",
        "    def init_hidden(self,batch_size):\n",
        "        # Hidden layer initialized to 0\n",
        "        return (torch.zeros(self.num_layers, self.embed_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layers, self.embed_size, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D31fvSpRF__P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Setup Training, Validation and Test sets**"
      ]
    },
    {
      "metadata": {
        "id": "MXISFWxuG4ZZ",
        "colab_type": "code",
        "outputId": "064e985c-1b99-48d8-ac8f-d7200b3c72f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "data, labels = get_model_inputs(tokenized_corpus, word2idx, train_labels, max_len)\n",
        "\n",
        "print(\"Data Tensor shape: {}, Labels shape: {}\". format(data.shape, labels.shape))\n",
        "\n",
        "###\n",
        "X_training, y_training, X_test, y_test = get_train_test_dataset(data, labels)\n",
        "X_train, y_train, X_val, y_val = get_train_test_dataset(X_training, y_training)\n",
        "###\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "# Batchify the training set\n",
        "X_train_batched = batchify(X_train, 128)\n",
        "print(\"X_train_batched\", X_train_batched.shape)\n",
        "\n",
        "# Batchify the validation set\n",
        "X_val_batched = batchify(X_val, 128)\n",
        "X_val_batched = batchify(X_val, 128)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_val_batched shape:\", X_val_batched.shape)\n",
        "\n",
        "# Batchify the set set\n",
        "X_test_batched = batchify(X_test, 128)\n",
        "X_test_batched = batchify(X_test, 128)\n",
        "print('X_test.shape:         {}'.format(X_test.shape))\n",
        "print('X_test_batched.shape: {}'.format(X_test_batched.shape))\n",
        "print('X_test_batched.shape: {}'.format(X_test_batched.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Tensor shape: torch.Size([4400, 81]), Labels shape: torch.Size([4400])\n",
            "X_train shape: torch.Size([3564, 81])\n",
            "X_train_batched torch.Size([128, 2187])\n",
            "X_val shape: torch.Size([396, 81])\n",
            "X_val_batched shape: torch.Size([128, 243])\n",
            "X_test.shape:         torch.Size([440, 81])\n",
            "X_test_batched.shape: torch.Size([128, 243])\n",
            "X_test_batched.shape: torch.Size([128, 243])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1jujj9qafyaq",
        "colab_type": "code",
        "outputId": "61bcb937-3eb6-4995-c671-c4514a0a3101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_train.shape: {}'.format(X_train.shape))\n",
        "\n",
        "# we will train for N epochs (N times the model will see all the data)\n",
        "epochs=30\n",
        "\n",
        "# the input dimension is the vocabulary size\n",
        "INPUT_DIM = len(word2idx)\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "# Number of RNN layers for LSTM\n",
        "NUM_RNN_LAYERS = 20\n",
        "\n",
        "#the outut dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Size 1 of X_train\n",
        "NUM_FEATURES = X_train.size(1)    # EMBEDDING DIM\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: torch.Size([3564, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-q0Wr6YzGMzi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Initialise, train and test Models**\n",
        "## **Model with Convolution Layer - LSTM Layer**\n",
        "## **Model with LSTM Layer - Convolution Layer**"
      ]
    },
    {
      "metadata": {
        "id": "4EZFhxvqJNxF",
        "colab_type": "code",
        "outputId": "63e8d56a-9cdf-49cc-c476-b6e46e2992bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_train.shape: {}'.format(X_train.shape))\n",
        "print('X_val.shape: {}'.format(X_val.shape))\n",
        "print('X_test.shape: {}'.format(X_test.shape))\n",
        "\n",
        "# Encoder_lstm\n",
        "# Model_lstm_conv\n",
        "model = Encoder_lstm(INPUT_DIM, NUM_FEATURES, HIDDEN_DIM, NUM_RNN_LAYERS).to(device)\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {}\".format(params))\n",
        "print(model)\n",
        "\n",
        "# Set up an Adam optimiser\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=1e-6)\n",
        "\n",
        "# We use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "def loss_fn(output, targets, loss_weights=None): \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    if loss_weights is not None:\n",
        "        criterion.weight=loss_weights\n",
        "    \n",
        "    return criterion.forward(output, targets)\n",
        "\n",
        "print('X_train.shape: {}'.format(X_train.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: torch.Size([3564, 81])\n",
            "X_val.shape: torch.Size([396, 81])\n",
            "X_test.shape: torch.Size([440, 81])\n",
            "Total number of parameters is: 3756847\n",
            "Encoder_lstm(\n",
            "  (embedding): Embedding(7630, 81)\n",
            "  (dropout): Dropout2d(p=0.2)\n",
            "  (conv1): Conv1d(81, 128, kernel_size=(3,), stride=(1,))\n",
            "  (conv2): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
            "  (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
            "  (lstm): LSTM(75, 128, num_layers=20)\n",
            "  (fc1): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "X_train.shape: torch.Size([3564, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqBGl7aMeqVL",
        "colab_type": "code",
        "outputId": "04d10f70-8870-4e58-bf1e-f3565ef3b54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_train.shape: {}'.format(X_train.shape))\n",
        "print('y_train.shape: {}'.format(y_train.shape))\n",
        "print('NUM_FEATURES:  {}'.format(NUM_FEATURES))\n",
        "print('BATCH_SIZE:    {}'.format(BATCH_SIZE))\n",
        "first_loss = 0.0\n",
        "\n",
        "# Train part\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    losses = []\n",
        "    total_accuracy = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1_score = 0\n",
        "    # Initialise hidden weights every epoch to zeros\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    for i, (inputs, targets) in enumerate(getBatch(X_train_batched, y_train, NUM_FEATURES, BATCH_SIZE)):\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        hidden = (hidden[0].detach().to(device), hidden[1].detach().to(device))\n",
        "        # We zero the gradients as they are not removed automatically\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
        "        output, hidden = model(inputs, hidden)\n",
        "        output = output.squeeze(1)\n",
        "        \n",
        "        loss_weights = torch.ones(BATCH_SIZE)\n",
        "        \n",
        "        ones_idx = [i for i, x in enumerate(targets) if x == 1.0]\n",
        "        zeros_idx = [i for i, x in enumerate(targets) if x == 0.0]\n",
        "        \n",
        "        loss_weights[ones_idx] = 0.15\n",
        "        loss_weights[zeros_idx] = 0.85\n",
        "        loss_weights = loss_weights.to(device)\n",
        "\n",
        "        loss = loss_fn(output, targets, loss_weights)\n",
        "      \n",
        "#         print(loss)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # Get the prediction from the output of the model\n",
        "        predictions = torch.round(torch.sigmoid(output))\n",
        "\n",
        "        # Get the metrics for forward pass\n",
        "        acc, recall, precision, f1 = calculate_metrics(targets.cpu().numpy(), predictions.cpu().data.numpy())\n",
        "              \n",
        "        total_accuracy += acc\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1_score += f1\n",
        "\n",
        "        # Calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "        \n",
        "        # Clip gradient to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        \n",
        "        # Update the parameters using the gradients and optimizer algorithm \n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = sum(losses) / (i+1)\n",
        "\n",
        "    print('Epoch: {:02} | Train Loss: {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}'.format(epoch, epoch_loss, total_f1_score/(i+1), total_accuracy/(i+1), total_precision/(i+1), total_recall/(i+1)))\n",
        "    \n",
        "    \n",
        "    ### VALIDATION ###\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    hidden = model.init_hidden(X_val.shape[0])\n",
        "    with torch.no_grad():\n",
        "      \n",
        "        inputs = X_val\n",
        "        targets = y_val\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        hidden = (hidden[0].detach().to(device), hidden[1].detach().to(device))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, hidden = model(inputs, hidden)\n",
        "        output = output.squeeze(1)\n",
        "        \n",
        "        loss_weights = torch.ones(X_val.shape[0])\n",
        "        \n",
        "        ones_idx = [i for i, x in enumerate(y_val) if x == 1.0]\n",
        "        zeros_idx = [i for i, x in enumerate(y_val) if x == 0.0]\n",
        "        \n",
        "        loss_weights[ones_idx] = 0.15\n",
        "        loss_weights[zeros_idx] = 0.85\n",
        "        loss_weights = loss_weights.to(device)\n",
        "\n",
        "        loss = loss_fn(output, targets, loss_weights)\n",
        "        \n",
        "        # Save the first loss to compare in following epochs, to stop overfitting\n",
        "        if epoch == 1:\n",
        "            first_loss = loss\n",
        "        elif (loss - first_loss) > 0.15:\n",
        "            print(\"Loss: {}, First loss: {}\".format(loss, first_loss))\n",
        "            print(\"Loss is going up above your tolerance. So we stop training\")\n",
        "            break\n",
        "        predictions = torch.round(torch.sigmoid(output))\n",
        "        \n",
        "        acc, recall, precision, f1 = calculate_metrics(y_val.numpy(), predictions.cpu().data.numpy())\n",
        "        \n",
        "        print('            Validation Loss:     {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}'.format(loss, f1, acc, precision, recall))\n",
        "        "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape: torch.Size([3564, 81])\n",
            "y_train.shape: torch.Size([3564])\n",
            "NUM_FEATURES:  81\n",
            "BATCH_SIZE:    128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Train Loss: 0.159 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 02 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.150 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 03 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 04 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 05 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 06 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 07 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 08 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 09 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 10 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 11 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 12 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 13 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 14 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 15 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 16 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 17 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 18 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 19 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 20 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 21 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 22 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 23 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 24 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 25 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 26 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 27 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 28 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 29 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n",
            "Epoch: 30 | Train Loss: 0.158 | F1: 0.4691 | Accuracy: 0.8840 | Precision: 0.8840 | Recall: 1.0000\n",
            "            Validation Loss:     0.148 | F1: 0.4734 | Accuracy: 0.8990 | Precision: 0.8990 | Recall: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONQOMYR_w0tW",
        "colab_type": "code",
        "outputId": "a402c015-c71c-498d-885e-368480441d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "print('X_test.shape: {}'.format(X_test.shape))\n",
        "print('y_test.shape: {}'.format(y_test.shape))\n",
        "\n",
        "# Check metrics on test set\n",
        "model.eval()\n",
        "hidden = model.init_hidden(BATCH_SIZE)\n",
        "with torch.no_grad():\n",
        "    inputs = X_test\n",
        "    targets = y_test\n",
        "    \n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    hidden = (hidden[0].detach().to(device), hidden[1].detach().to(device))\n",
        "    \n",
        "    output_full, hidden = model(inputs, hidden)\n",
        "    output_full = output_full.squeeze(1)\n",
        "    \n",
        "    loss_weights = torch.ones(y_test.shape[0])\n",
        "        \n",
        "    ones_idx = [i for i, x in enumerate(y_test) if x == 1.0]\n",
        "    zeros_idx = [i for i, x in enumerate(y_test) if x == 0.0]\n",
        "\n",
        "    loss_weights[ones_idx] = 0.15\n",
        "    loss_weights[zeros_idx] = 0.85\n",
        "    loss_weights = loss_weights.to(device)\n",
        "\n",
        "    loss_full = loss_fn(output_full, targets, loss_weights)\n",
        "\n",
        "    predictions_full = torch.round(torch.sigmoid(output_full))\n",
        "        \n",
        "    acc_full, recall_full, precision_full, f1_full = calculate_metrics(y_test.numpy(), predictions_full.cpu().data.numpy())\n",
        "        \n",
        "print('FULL TEST SET: Test Loss: {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}'.format(loss_full, f1_full, acc_full, precision_full, recall_full))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test.shape: torch.Size([440, 81])\n",
            "y_test.shape: torch.Size([440])\n",
            "FULL TEST SET: Test Loss: 0.175 | F1: 0.4614 | Accuracy: 0.8568 | Precision: 0.8568 | Recall: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "R1083fG5PwZL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exporting labels for the submission\n",
        "Run the given test data which does not contain any labels\n",
        "\n",
        "## Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "Fkv858wXR2wW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_inputs_test(tokenized_corpus, word2idx, max_len):\n",
        "\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "\n",
        "  data_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  \n",
        "  data_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # we fill it with our vectorized sentences \n",
        "  \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, data_lengths)):\n",
        "    data_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "  \n",
        "  return data_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ta4q3Si3QYoV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc46629a-90d1-4353-96be-2ac61cf1f60b"
      },
      "cell_type": "code",
      "source": [
        "# Path files for training and test data\n",
        "test_task_b_path = 'testset-taskb.tsv'\n",
        "\n",
        "test_tweets = []\n",
        "test_ids = []\n",
        "with open(test_task_b_path) as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    print(reader.fieldnames)\n",
        "    for row in reader:\n",
        "        test_ids.append(row['id'])\n",
        "        test_tweets.append(row['tweet'])\n",
        "\n",
        "test = preprocess_text(test_tweets)\n",
        "\n",
        "\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(test)\n",
        "print(\"Tokenized corpus size:\", len(tokenized_corpus))\n",
        "\n",
        "# sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "# max_len = np.max(np.array(sent_lengths))\n",
        "\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "print(\"Word2Idx size:\", len(word2idx))\n",
        "\n",
        "data = get_model_inputs_test(tokenized_corpus, word2idx, max_len)\n",
        "print(\"Data shape:\", data.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id', 'tweet']\n",
            "Tokenized corpus size: 240\n",
            "Word2Idx size: 1647\n",
            "Data shape: torch.Size([240, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TihuuL5-T8JB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the input dimension is the vocabulary size\n",
        "INPUT_DIM = len(word2idx)\n",
        "\n",
        "# Size 1 of X_test_batched\n",
        "NUM_FEATURES = data.size(1)     # Embedding dimension"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNvUBAKRPy-q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Forward Pass\n"
      ]
    },
    {
      "metadata": {
        "id": "5UO2qkeyPa-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check metrics on test set\n",
        "model.eval()\n",
        "hidden = model.init_hidden(data.shape[0])\n",
        "with torch.no_grad():\n",
        "    data = data.to(device)\n",
        "    hidden = (hidden[0].detach().to(device), hidden[1].detach().to(device))\n",
        "    # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
        "    _predictions, hidden = model(data, hidden)\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(_predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uB1sg51TP4By",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write to CSV file"
      ]
    },
    {
      "metadata": {
        "id": "pAwPRJ7pPd_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'TIN' == 1.\n",
        "# 'UNT' == 0.\n",
        "output = np.array([\"empty\" for x in range(predictions.shape[0])])\n",
        "        \n",
        "tin_idx = [i for i, x in enumerate(predictions) if x == 1.0]\n",
        "unt_idx = [i for i, x in enumerate(predictions) if x == 0.0]\n",
        "\n",
        "output[tin_idx] = 'TIN'\n",
        "output[unt_idx] = 'UNT'\n",
        "\n",
        "output = np.vstack((test_ids, output)).T\n",
        "\n",
        "# write to csv file\n",
        "with open('taskb_submission.csv', 'w') as myFile:  \n",
        "    writer = csv.writer(myFile)\n",
        "    writer.writerows(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOo6ljGpacUX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}