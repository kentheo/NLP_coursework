{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_task_A.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ykpGgBeqJNwh",
        "colab_type": "code",
        "outputId": "2baeef2d-c57b-4bbc-b5cd-ae47e65e679e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from keras.preprocessing import text, sequence\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f055c679130>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "g72boCXeJWru",
        "colab_type": "code",
        "outputId": "4a667b4d-68e3-4709-fb15-d061f48aeb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1499
        }
      },
      "cell_type": "code",
      "source": [
        "# Download Training Data\n",
        "!wget https://competitions.codalab.org/my/datasets/download/60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
        "!unzip 60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
        "!unzip training-v1.zip\n",
        "!unzip trial-data.zip\n",
        "!rm '60e40c68-a85d-4320-bef1-d2fe26bb45ca'\n",
        "!rm training-v1.zip\n",
        "!rm trial-data.zip\n",
        "!rm -r __MACOSX\n",
        "\n",
        "# Download Test Data\n",
        "!wget https://competitions.codalab.org/my/datasets/download/5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "!unzip 5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "!rm 5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
        "\n",
        "!wget https://competitions.codalab.org/my/datasets/download/bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "!unzip bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "!rm bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
        "\n",
        "!wget https://competitions.codalab.org/my/datasets/download/38273e56-2ab0-4773-82bf-95aec51bba69\n",
        "!unzip 38273e56-2ab0-4773-82bf-95aec51bba69\n",
        "!rm 38273e56-2ab0-4773-82bf-95aec51bba69"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-01 22:32:36--  https://competitions.codalab.org/my/datasets/download/60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/787f6/start-kit.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=a00e46f1043d3a6b0e95bb649ea11ed0fbeb5453cbd22b3b372f1bccdaaecd7f&X-Amz-Date=20190301T223232Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 22:32:37--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/787f6/start-kit.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=a00e46f1043d3a6b0e95bb649ea11ed0fbeb5453cbd22b3b372f1bccdaaecd7f&X-Amz-Date=20190301T223232Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 796236 (778K) [application/zip]\n",
            "Saving to: ‘60e40c68-a85d-4320-bef1-d2fe26bb45ca’\n",
            "\n",
            "60e40c68-a85d-4320- 100%[===================>] 777.57K  1.08MB/s    in 0.7s    \n",
            "\n",
            "2019-03-01 22:32:38 (1.08 MB/s) - ‘60e40c68-a85d-4320-bef1-d2fe26bb45ca’ saved [796236/796236]\n",
            "\n",
            "Archive:  60e40c68-a85d-4320-bef1-d2fe26bb45ca\n",
            " extracting: training-v1.zip         \n",
            "  inflating: trial-data.zip          \n",
            "Archive:  training-v1.zip\n",
            "  inflating: offenseval-training-v1.tsv  \n",
            "  inflating: readme-trainingset-v1.txt  \n",
            "  inflating: offenseval-annotation.txt  \n",
            "Archive:  trial-data.zip\n",
            "  inflating: OffensEval-READMEv1.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._OffensEval-READMEv1.txt  \n",
            "  inflating: offenseval-trial.txt    \n",
            "  inflating: __MACOSX/._offenseval-trial.txt  \n",
            "--2019-03-01 22:32:48--  https://competitions.codalab.org/my/datasets/download/5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/7b71f/Test%20A%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=bc70a5d76612d7ae29fa7bdb281859b49787a24f7935053a49146bdfeb82915f&X-Amz-Date=20190301T223244Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 22:32:49--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/7b71f/Test%20A%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=bc70a5d76612d7ae29fa7bdb281859b49787a24f7935053a49146bdfeb82915f&X-Amz-Date=20190301T223244Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63867 (62K) [application/zip]\n",
            "Saving to: ‘5cac0f56-bb6d-40fa-8041-caf8aa13d09d’\n",
            "\n",
            "5cac0f56-bb6d-40fa- 100%[===================>]  62.37K   222KB/s    in 0.3s    \n",
            "\n",
            "2019-03-01 22:32:49 (222 KB/s) - ‘5cac0f56-bb6d-40fa-8041-caf8aa13d09d’ saved [63867/63867]\n",
            "\n",
            "Archive:  5cac0f56-bb6d-40fa-8041-caf8aa13d09d\n",
            "  inflating: testset-taska.tsv       \n",
            "  inflating: readme-testsetA-v1.txt  \n",
            "--2019-03-01 22:32:53--  https://competitions.codalab.org/my/datasets/download/bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4c1e9/Test%20B%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ad40b5258247e9aa054d76997d158cafe98dd7efa3121eacc35db095dbbde22c&X-Amz-Date=20190301T223249Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 22:32:53--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4c1e9/Test%20B%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=ad40b5258247e9aa054d76997d158cafe98dd7efa3121eacc35db095dbbde22c&X-Amz-Date=20190301T223249Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18670 (18K) [application/zip]\n",
            "Saving to: ‘bb373027-c8b7-48ab-9729-b1ab3fb51c17’\n",
            "\n",
            "bb373027-c8b7-48ab- 100%[===================>]  18.23K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-03-01 22:32:54 (130 KB/s) - ‘bb373027-c8b7-48ab-9729-b1ab3fb51c17’ saved [18670/18670]\n",
            "\n",
            "Archive:  bb373027-c8b7-48ab-9729-b1ab3fb51c17\n",
            "  inflating: readme-testsetB-v1.txt  \n",
            "  inflating: testset-taskb.tsv       \n",
            "--2019-03-01 22:32:58--  https://competitions.codalab.org/my/datasets/download/38273e56-2ab0-4773-82bf-95aec51bba69\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 134.158.75.178\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|134.158.75.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4003e/Test%20C%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e0c159290f0814a833007b551dbb70281a198976e2e015acd97ea130e8fb6920&X-Amz-Date=20190301T223254Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2019-03-01 22:32:58--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/4003e/Test%20C%20Release.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=e0c159290f0814a833007b551dbb70281a198976e2e015acd97ea130e8fb6920&X-Amz-Date=20190301T223254Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20190301%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17439 (17K) [application/zip]\n",
            "Saving to: ‘38273e56-2ab0-4773-82bf-95aec51bba69’\n",
            "\n",
            "38273e56-2ab0-4773- 100%[===================>]  17.03K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-03-01 22:32:59 (122 KB/s) - ‘38273e56-2ab0-4773-82bf-95aec51bba69’ saved [17439/17439]\n",
            "\n",
            "Archive:  38273e56-2ab0-4773-82bf-95aec51bba69\n",
            "  inflating: test_set_taskc.tsv      \n",
            "  inflating: readme-testsetc-v1.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxFLYys0MegL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_tokenized_corpus(corpus):\n",
        "\n",
        "  tokenized_corpus = []\n",
        "\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        " \n",
        "  return tokenized_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoQqdbOcMgcV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the placeholder token\n",
        "  word2idx['<pad>'] = 0\n",
        " \n",
        "  return word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbH4PMDRMj3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
        "\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "\n",
        "  data_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  \n",
        "  data_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # we fill it with our vectorized sentences \n",
        "  \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, data_lengths)):\n",
        "    data_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "\n",
        "  label_tensor = torch.Tensor(labels)\n",
        "  \n",
        "  return data_tensor, label_tensor\n",
        "\n",
        "\n",
        "def get_model_inputs_test(tokenized_corpus, word2idx, max_len):\n",
        "\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "\n",
        "  data_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
        "  \n",
        "  data_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # we fill it with our vectorized sentences \n",
        "  \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, data_lengths)):\n",
        "    data_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
        "  \n",
        "  return data_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2LG_Wh84cHrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchify(data, batch_size):\n",
        "    '''\n",
        "    This function will divide data into batches\n",
        "    '''\n",
        "    \n",
        "    nbatch = data.size(0) // batch_size # divide the dataset into batches parts.\n",
        "\n",
        "    data = data.narrow(0, 0, nbatch * batch_size) # extra elments that dont fit the batches\n",
        "    \n",
        "    data = data.view(batch_size, -1).contiguous() # Evenly divide into all batches\n",
        "   \n",
        "    if USE_CUDA:\n",
        "        data = data.cuda()\n",
        "        \n",
        "    return data\n",
        "  \n",
        "\n",
        "def getBatch(data, labels, seq_length, batch_size):\n",
        "    '''\n",
        "    This function gives us batches for training\n",
        "    * this is language modelling, our targets will be the next words\n",
        "    * this function gives partial sequence samples\n",
        "    '''\n",
        "    for i in range(0, data.size(1) - seq_length, seq_length):\n",
        "        inputs = data[:, i: i + seq_length]\n",
        "#         print(\"inputs\", inputs)\n",
        "        targets = labels[i: i + batch_size]\n",
        "      #         targets = Variable(data[:, (i + 1): (i + 1) + seq_length].contiguous())\n",
        "        yield (inputs, targets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m3pe8ag5OTYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_metrics(real, preds):\n",
        "    acc = accuracy_score(real, preds)\n",
        "    recall = recall_score(real, preds)\n",
        "    precision = precision_score(real, preds)\n",
        "    f1 = f1_score(real, preds, average='macro')\n",
        "    \n",
        "    return acc, recall, precision, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "va6YByaCS2u_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_train_test_dataset(X, y, test_split=0.1, shuffle_dataset=True, random_seed=None):\n",
        "  indices = list(range(X.shape[0]))\n",
        "  test_split_idx = int(np.floor(test_split * X.shape[0]))\n",
        "\n",
        "  if shuffle_dataset and random_seed is not None:\n",
        "      np.random.seed(random_seed)\n",
        "      np.random.shuffle(indices)\n",
        "  elif shuffle_dataset:\n",
        "      np.random.seed()\n",
        "      np.random.shuffle(indices)\n",
        "  train_indices, test_indices = indices[test_split_idx:], indices[:test_split_idx]\n",
        "\n",
        "  X_train_ = X[train_indices]\n",
        "  X_test_ = X[test_indices]\n",
        "  y_train_ = y[train_indices]\n",
        "  y_test_ = y[test_indices]\n",
        "  \n",
        "  return X_train_, y_train_, X_test_, y_test_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xErMzIaT4nUI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text_list):\n",
        "\n",
        "    output = []\n",
        "    for text in text_list:\n",
        "        ## Remove puncuation\n",
        "        text = text.translate(string.punctuation)\n",
        "\n",
        "        ## Convert words to lower case and split them\n",
        "        text = text.lower().split()\n",
        "\n",
        "        ## Remove stop words\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "\n",
        "        text = \" \".join(text)\n",
        "\n",
        "        # Clean the text\n",
        "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "        text = re.sub(r\"what's\", \"what is \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"i'm\", \"i am \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "        text = re.sub(r\",\", \" \", text)\n",
        "        text = re.sub(r\"\\.\", \" \", text)\n",
        "        text = re.sub(r\"!\", \" ! \", text)\n",
        "        text = re.sub(r\"\\/\", \" \", text)\n",
        "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "        text = re.sub(r\"\\+\", \" + \", text)\n",
        "        text = re.sub(r\"\\-\", \" - \", text)\n",
        "        text = re.sub(r\"\\=\", \" = \", text)\n",
        "        text = re.sub(r\"'\", \" \", text)\n",
        "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "        text = re.sub(r\":\", \" : \", text)\n",
        "        text = re.sub(r\" e g \", \" eg \", text)\n",
        "        text = re.sub(r\" b g \", \" bg \", text)\n",
        "        text = re.sub(r\" u s \", \" american \", text)\n",
        "        text = re.sub(r\"\\0s\", \"0\", text)\n",
        "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "        text = re.sub(r\"e - mail\", \"email\", text)\n",
        "        text = re.sub(r\"j k\", \"jk\", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "        text = re.sub(r\"[^\\w]\", \" \", text)\n",
        "\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "        \n",
        "        output.append(text)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WSzsTFr6TUja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "l9Cq-ce0OMEw",
        "colab_type": "code",
        "outputId": "1e39259b-c932-491d-df35-5ab40e802842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#######################\n",
        "### DATA PROCESSING ###\n",
        "#######################\n",
        "# Path files for training and test data\n",
        "train_path = 'offenseval-training-v1.tsv'\n",
        "test_task_a_path = 'testset_taska.tsv'\n",
        "test_task_b_path = 'testset_taskb.tsv'\n",
        "test_task_c_path = 'test_set_taskc.tsv'\n",
        "\n",
        "tweets = []\n",
        "subtask_a, subtask_b, subtask_c = [], [], []\n",
        "# filename = '/Users/battilanast/polybox/Dropbox_Archive/My_Schoolstuff/ETHZ_Archive/Informatik/ICL_ST19/Natural_Language_Processing/Coursework/data/start-kit/training-v1/offenseval-training-v1.tsv'\n",
        "with open(train_path) as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    print(reader.fieldnames)\n",
        "    for row in reader:\n",
        "        tweets.append(row['tweet'])\n",
        "        subtask_a.append(row['subtask_a'])\n",
        "        subtask_b.append(row['subtask_b'])\n",
        "        subtask_c.append(row['subtask_c'])\n",
        "\n",
        "\n",
        "\n",
        "tweets_clean = clean_text(tweets)\n",
        "train = tweets_clean\n",
        "\n",
        "train_labels = np.array(subtask_a)\n",
        "no_off = len(train_labels[train_labels == 'OFF'])\n",
        "no_not = len(train_labels[train_labels == 'NOT'])\n",
        "weight_not = no_not / (no_off + no_not)\n",
        "weight_off = no_off / (no_off + no_not)\n",
        "train_labels[train_labels == 'OFF'] = 1.\n",
        "train_labels[train_labels == 'NOT'] = 0.\n",
        "\n",
        "\n",
        "train_labels = train_labels.astype(np.float)\n",
        "# print(train_labels.sum())\n",
        "\n",
        "print(\"Length of train set: {}, Length of train labels: {}\".format(len(train), len(train_labels)))\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(train)\n",
        "print(\"Tokenized corpus size:\", len(tokenized_corpus))\n",
        "\n",
        "sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "max_len = np.max(np.array(sent_lengths))\n",
        "\n",
        "        \n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "print(\"Word2Idx size:\", len(word2idx))\n",
        "\n",
        "\n",
        "data, labels = get_model_inputs(tokenized_corpus, word2idx, train_labels, max_len)\n",
        "\n",
        "print(\"Train Tensor shape: {}, Train labels shape: {}\". format(data.shape, labels.shape))\n",
        "\n",
        "###\n",
        "X_training, y_training, X_test, y_test = get_train_test_dataset(data, labels)\n",
        "X_train, y_train, X_val, y_val = get_train_test_dataset(X_training, y_training)\n",
        "###\n",
        "\n",
        "### splitting validation and test set into offensive and non-offensive\n",
        "nots_test_idx = [i for i, x in enumerate(y_test) if x == 0.0]\n",
        "offs_test_idx = [i for i, x in enumerate(y_test) if x == 1.0]\n",
        "nots_val_idx = [i for i, x in enumerate(y_val) if x == 0.0]\n",
        "offs_val_idx = [i for i, x in enumerate(y_val) if x == 1.0]\n",
        "\n",
        "X_test_off = X_test[offs_test_idx]\n",
        "X_test_not = X_test[nots_test_idx]\n",
        "y_test_off = y_test[offs_test_idx]\n",
        "y_test_not = y_test[nots_test_idx]\n",
        "\n",
        "X_val_off = X_val[offs_val_idx]\n",
        "X_val_not = X_val[nots_val_idx]\n",
        "y_val_off = y_val[offs_val_idx]\n",
        "y_val_not = y_val[nots_val_idx]\n",
        "###\n",
        "\n",
        "### Steven end ###\n",
        "    \n",
        "# Batchify the training set\n",
        "X_train_batched = batchify(X_train, 128)\n",
        "print(\"X_train\", X_train.shape)\n",
        "print(X_train_batched.shape)\n",
        "\n",
        "# Batchify the validation set\n",
        "X_val_off_batched = batchify(X_val_off, 128)\n",
        "X_val_not_batched = batchify(X_val_not, 128)\n",
        "X_val_batched = batchify(X_val, 128)\n",
        "print(\"X_val\", X_val.shape)\n",
        "print(\"X_val_not_batched\", X_val_not_batched.shape)\n",
        "print(\"X_val_batched\", X_val_batched.shape)\n",
        "\n",
        "# Batchify the set set\n",
        "X_test_off_batched = batchify(X_test_off, 128)\n",
        "X_test_not_batched = batchify(X_test_not, 128)\n",
        "X_test_batched = batchify(X_test, 128)\n",
        "print('X_test.shape:         {}'.format(X_test.shape))\n",
        "print('X_test_off_batched.shape: {}'.format(X_test_off_batched.shape))\n",
        "print('X_test_not_batched.shape: {}'.format(X_test_not_batched.shape))\n",
        "print('X_test_batched.shape: {}'.format(X_test_batched.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q_d5n84DX0pi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### download glove dataset\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p5JnK1jrX0DD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('glove.6B.300d.txt')):\n",
        "    val = line.split()\n",
        "    embeddings_index[val[0]] = np.asarray(val[1:], dtype='float32')\n",
        "\n",
        "for word, i in word2idx.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nltiF1PsTEvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "nNa9yD-0JNxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##############\n",
        "### MODELS ###\n",
        "##############\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.n_layers = 1\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        ## Embedding Layer, Add parameter \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)        \n",
        "        self.fc1 = nn.Linear(hidden_size, 1)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x, h, is_training=False):\n",
        "        x = self.embedding(x)\n",
        "        x, (h, c) = self.lstm(x)\n",
        "        # Take the average of the embeddings\n",
        "#         x = x.mean(1)\n",
        "        x, _ = torch.max(x, 1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "    \n",
        "        return x, h\n",
        "\n",
        "    def init_weight(self):\n",
        "      \n",
        "        # We initialize the network to uniform weights in the range (-0.1, 0.1)\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc1.bias.data.zero_()\n",
        "#         self.fc2.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.fc2.bias.data.zero_()\n",
        "        \n",
        "    def init_hidden(self,batch_size):\n",
        "        # This function gives us the hidden layer initialized to 0 \n",
        "        # Refer lecture slide\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "\n",
        "\n",
        "class Model_lstm(nn.Module):\n",
        "    def __init__(self, max_features, embed_size):\n",
        "        super(Model_lstm, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, 20)        \n",
        "        self.fully_connected = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "        x = x.mean(1)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "      \n",
        "      \n",
        "class Model_conv_lstm_glove(nn.Module):\n",
        "    def __init__(self, max_features, embed_size, hidden_size):\n",
        "        super(Model_conv_lstm_glove, self).__init__()\n",
        "        \n",
        "        self.n_layers = 1\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "        self.embedding.weight = nn.Parameter(et)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.convolution = nn.Conv1d(embed_size, 5, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pooling = nn.MaxPool1d(3)\n",
        "        self.lstm = nn.LSTM(99, 20, 2)        \n",
        "        self.fully_connected = nn.Linear(20, 1)\n",
        "        \n",
        "#         self.hidden = self.init_hidden()\n",
        "\n",
        "    def forward(self, x, hidden_state, cell_state):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.convolution(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pooling(x)\n",
        "        print('x.shape:      {}'.format(x.shape))\n",
        "        print('hidden.shape: {}'.format(hidden.shape))\n",
        "#         x, (hidden_state, cell_state) = self.lstm(x, hidden)\n",
        "        x, (hidden_state, cell_state) = self.lstm(x, hidden_state, cell_state)\n",
        "        print('hidden after:', hidden_state.shape)\n",
        "        # Take the average of the embeddings\n",
        "        x, _ = torch.max(x, 1)\n",
        "#         x = x.mean(1)\n",
        "        x = self.fully_connected(x)\n",
        "        return x, hidden_state\n",
        "\n",
        "    def init_weight(self):\n",
        "      \n",
        "        # We initialize the network to uniform weights in the range (-0.1, 0.1)\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc1.bias.data.zero_()\n",
        "#         self.fc2.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.fc2.bias.data.zero_()\n",
        "        \n",
        "    def init_hidden(self,batch_size):\n",
        "        # This function gives us the hidden layer initialized to 0 \n",
        "        # Refer lecture slide\n",
        "#         weight = next(self.parameters())\n",
        "#         return weight.new_zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "        return torch.zeros(2, batch_size, 20), torch.zeros(2, batch_size, 20)\n",
        "\n",
        "\n",
        "class Model_conv_lstm(nn.Module):\n",
        "    def __init__(self, max_features, embed_size):\n",
        "        super(Model_conv_lstm, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.convolution = nn.Conv1d(82, 128, 103)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pooling = nn.MaxPool1d(3)\n",
        "        self.lstm = nn.LSTM(1, 20)        \n",
        "        self.fully_connected = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.convolution(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pooling(x)\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "\n",
        "        x, _ = torch.max(x, 1)\n",
        "\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class Model_lstm_conv(nn.Module):\n",
        "    def __init__(self, max_features, embed_size):\n",
        "        super(Model_lstm_conv, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "#         self.dropout = nn.Dropout2d(0.2)\n",
        "        self.lstm = nn.LSTM(103, 20, dropout=0.5) \n",
        "        self.convolution = nn.Conv1d(82, 128, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pooling = nn.MaxPool1d(3)       \n",
        "        self.fully_connected = nn.Linear(6, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "#         x = self.dropout(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.convolution(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pooling(x)\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "        x = x.mean(1)\n",
        "#         x, _ = torch.max(x, 1)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model_conv_lstm_2linear(nn.Module):\n",
        "    def __init__(self, max_features, embed_size):\n",
        "        super(Model_conv_lstm_2linear, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "        self.embedding.weight = nn.Parameter(et)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.convolution = nn.Conv1d(82, 128, 103)\n",
        "        self.max_pooling = nn.MaxPool1d(3)\n",
        "        self.lstm = nn.LSTM(1, 20)\n",
        "        self.fully_connected0 = nn.Linear(20, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fully_connected1 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.convolution(x)\n",
        "\n",
        "        x = self.max_pooling(x)\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        x = self.fully_connected0(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "\n",
        "        x, _ = torch.max(x, 1)\n",
        "\n",
        "        x = self.fully_connected1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Model_conv_gru_2linear(nn.Module):\n",
        "    def __init__(self, max_features, embed_size):\n",
        "        super(Model_conv_gru_2linear, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        et = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "        self.embedding.weight = nn.Parameter(et)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "        self.convolution = nn.Conv1d(82, 128, 103)\n",
        "        self.max_pooling = nn.MaxPool1d(3)\n",
        "        self.gru = nn.GRU(66, 20, dropout=0.5)\n",
        "        self.fully_connected0 = nn.Linear(20, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fully_connected1 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.convolution(x)\n",
        "\n",
        "        x = self.max_pooling(x)\n",
        "\n",
        "        x, _ = self.gru(x)\n",
        "\n",
        "        x = self.fully_connected0(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        \n",
        "        # Take the average of the embeddings\n",
        "\n",
        "        x, _ = torch.max(x, 1)\n",
        "\n",
        "        x = self.fully_connected1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyGJ_3DCe_SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(output, target):\n",
        " \n",
        "    output = torch.round(torch.sigmoid(output))\n",
        "    correct = (output == target).float()\n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# we will train for N epochs (N times the model will see all the data)\n",
        "epochs=10\n",
        "\n",
        "# the input dimension is the vocabulary size\n",
        "INPUT_DIM = len(word2idx)\n",
        "\n",
        "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
        "EMBEDDING_DIM = 103\n",
        "\n",
        "# dimensionality of the output of the second hidden layer\n",
        "HIDDEN_DIM = 20\n",
        "\n",
        "#the outut dimension is the number of classes, 1 for binary classification\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# Size 1 of X_train\n",
        "NUM_FEATURES = X_train.size(1)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UduX64VnxYBf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('INPUT_DIM: {}'.format(INPUT_DIM))\n",
        "print('X_train_batched.shape: {}'.format(X_train_batched.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EZFhxvqJNxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will use the existing dimensions for now\n",
        "# model = Model(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "model = Model_conv_lstm_glove(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "# model = Model_conv_lstm(INPUT_DIM, EMBEDDING_DIM)\n",
        "# model = Model_lstm_conv(INPUT_DIM, EMBEDDING_DIM)\n",
        "# model = Model_lstm(INPUT_DIM, EMBEDDING_DIM)\n",
        "# model = Model_conv_gru_2linear(INPUT_DIM, EMBEDDING_DIM)\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total number of parameters is: {}\".format(params))\n",
        "print(model)\n",
        "\n",
        "# we use the stochastic gradient descent (SGD) optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "# # we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "# #Recall we did not apply any activation to our output layer, we need to make our outputs look like probality.\n",
        "# loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
        "def loss_fn(output, targets, loss_weights=None): \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    if loss_weights is not None:\n",
        "        criterion.weight=loss_weights\n",
        "    \n",
        "    return criterion.forward(output, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3mDthlhTNMo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "YqBGl7aMeqVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################\n",
        "### TRAINING ###\n",
        "################\n",
        "\n",
        "#to ensure the dropout (exlained later) is \"turned on\" while training\n",
        "#good practice to include even if do not use here\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1, epochs+1):\n",
        "    total_loss = 0\n",
        "    losses = []\n",
        "#     hidden = model.init_hidden(BATCH_SIZE)\n",
        "    total_accuracy = 0\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    total_f1_score = 0\n",
        "#     print(\"Hidden:\", hidden.shape)\n",
        "    for i, (inputs, targets) in enumerate(getBatch(X_train_batched, y_train, NUM_FEATURES, BATCH_SIZE)):\n",
        "        \n",
        "#         inputs, targets = batch\n",
        "#         print(\"Inputs size: {}, Targets size: {}\".format(inputs.shape, targets.shape))\n",
        "        \n",
        "        # we are detaching the hidden state from the \n",
        "        # computational graph to prevent backpropping \n",
        "        # entirely to previous batches \n",
        "#         hidden = hidden.detach()\n",
        "        \n",
        "        # we zero the gradients as they are not removed automatically\n",
        "        model.zero_grad()\n",
        "      \n",
        "        # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
        "        output = model(inputs)\n",
        "        output = output.squeeze()\n",
        "\n",
        "        loss_weights = torch.ones(targets.shape[0])\n",
        "\n",
        "        off_idx = [i for i, x in enumerate(targets) if x == 1.0]\n",
        "        not_idx = [i for i, x in enumerate(targets) if x == 0.0]\n",
        "\n",
        "        loss_weights[off_idx] = weight_off\n",
        "        loss_weights[not_idx] = weight_not\n",
        "\n",
        "        loss = loss_fn(output, targets, loss_weights)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        acc = accuracy(output, targets)\n",
        "        \n",
        "        predictions = torch.round(torch.sigmoid(output))\n",
        "        acc_sb, rec_sb, prec_sb, f1_sb = calculate_metrics(targets.numpy(), predictions.data.numpy())\n",
        "        \n",
        "        total_accuracy += acc_sb\n",
        "        total_precision += prec_sb\n",
        "        total_recall += rec_sb\n",
        "        total_f1_score += f1_sb\n",
        "        \n",
        "\n",
        "        #calculate the gradient of each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        #update the parameters using the gradients and optimizer algorithm \n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss = loss.item()\n",
        "        epoch_acc = acc\n",
        "\n",
        "    print('Epoch: {:02} | Train Loss:      {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}'.format(epoch, np.array(losses).mean(), total_f1_score/(i+1), total_accuracy/(i+1), total_precision/(i+1), total_recall/(i+1)))\n",
        "    \n",
        "\n",
        "# VALIDATION\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X_val)\n",
        "        output = output.squeeze(1)\n",
        "        \n",
        "        loss_weights = torch.ones(y_val.shape[0])\n",
        "\n",
        "        off_idx = [i for i, x in enumerate(y_val) if x == 1.0]\n",
        "        not_idx = [i for i, x in enumerate(y_val) if x == 0.0]\n",
        "\n",
        "        loss_weights[off_idx] = weight_off\n",
        "        loss_weights[not_idx] = weight_not\n",
        "\n",
        "        loss = loss_fn(output, y_val, loss_weights)\n",
        "        \n",
        "        # Save the first loss to compare in following epochs, to stop overfitting\n",
        "        if epoch == 1:\n",
        "            first_loss = loss\n",
        "        elif (loss - first_loss) > 0.15:\n",
        "            print(\"Loss: {}, First loss: {}\".format(loss, first_loss))\n",
        "            print(\"Loss is going up above your tolerance. So we stop training\")\n",
        "            break\n",
        "        predictions = torch.round(torch.sigmoid(output))\n",
        "        \n",
        "        acc, recall, precision, f1 = calculate_metrics(y_val.numpy(), predictions.data.numpy())\n",
        "        \n",
        "        print('            Validation Loss: {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}'.format(loss, f1, acc, precision, recall))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AImZ9YA-TQJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ]
    },
    {
      "metadata": {
        "id": "vBwWe_rcpv5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###############\n",
        "### TESTING ###\n",
        "###############\n",
        "\n",
        "# Check metrics on test set\n",
        "model.eval()\n",
        "# hidden = model.init_hidden(BATCH_SIZE)\n",
        "with torch.no_grad():\n",
        "    # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
        "    output_full = model(X_test)\n",
        "    output_full = output_full.squeeze(1)\n",
        "    \n",
        "    loss_weights = torch.ones(y_test.shape[0])\n",
        "        \n",
        "    off_idx = [i for i, x in enumerate(y_test) if x == 1.0]\n",
        "    not_idx = [i for i, x in enumerate(y_test) if x == 0.0]\n",
        "\n",
        "    loss_weights[off_idx] = weight_off\n",
        "    loss_weights[not_idx] = weight_not\n",
        "\n",
        "    loss_full = loss_fn(output_full, y_test, loss_weights)\n",
        "\n",
        "    predictions_full = torch.round(torch.sigmoid(output_full))\n",
        "        \n",
        "    acc_full, recall_full, precision_full, f1_full = calculate_metrics(y_test.numpy(), predictions_full.data.numpy())\n",
        "        \n",
        "print('FULL TEST SET: Test Loss: {:.3f} | F1: {:.4f} | Accuracy: {:.4f} | Precision: {:.4f} | Recall: {:.4f}\\n'.format(loss_full, f1_full, acc_full, precision_full, recall_full))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NjJnVFHrSToX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PASi5Fxzfkku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPbd2su_gn2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exporting labels for the submission\n",
        "Run the given test data which does not contain any labels\n",
        "\n",
        "## Data Processing"
      ]
    },
    {
      "metadata": {
        "id": "f4nrSjpqg21_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Path files for training and test data\n",
        "test_task_a_path = 'testset-taska.tsv'\n",
        "\n",
        "test_tweets = []\n",
        "test_ids = []\n",
        "with open(test_task_a_path) as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    print(reader.fieldnames)\n",
        "    for row in reader:\n",
        "        test_ids.append(row['id'])\n",
        "        test_tweets.append(row['tweet'])\n",
        "\n",
        "test = clean_text(test_tweets)\n",
        "\n",
        "\n",
        "\n",
        "tokenized_corpus = get_tokenized_corpus(test)\n",
        "print(\"Tokenized corpus size:\", len(tokenized_corpus))\n",
        "\n",
        "sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "# max_len = np.max(np.array(sent_lengths))\n",
        "\n",
        "word2idx = get_word2idx(tokenized_corpus)\n",
        "print(\"Word2Idx size:\", len(word2idx))\n",
        "\n",
        "data = get_model_inputs_test(tokenized_corpus, word2idx, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5OeMwFhA7gN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Forward Pass\n"
      ]
    },
    {
      "metadata": {
        "id": "dUEbZ9aDA6vt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check metrics on test set\n",
        "model.eval()\n",
        "# hidden = model.init_hidden(BATCH_SIZE)\n",
        "with torch.no_grad():\n",
        "    # squeeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1 \n",
        "    _predictions = model(data)\n",
        "    _predictions = _predictions.squeeze()\n",
        "\n",
        "    predictions = torch.round(torch.sigmoid(_predictions))\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gk_aBHDubLJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(predictions)\n",
        "print(predictions.shape)\n",
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RVIDOoYWK3tq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write to CSV file"
      ]
    },
    {
      "metadata": {
        "id": "hKFYPrC9BKTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'OFF' == 1.\n",
        "# 'NOT' == 0.\n",
        "output = np.array([\"empty\" for x in range(predictions.shape[0])])\n",
        "        \n",
        "off_idx = [i for i, x in enumerate(predictions) if x == 1.0]\n",
        "not_idx = [i for i, x in enumerate(predictions) if x == 0.0]\n",
        "\n",
        "output[off_idx] = 'OFF'\n",
        "output[not_idx] = 'NOT'\n",
        "\n",
        "print(type(test_ids))\n",
        "print(type(output))\n",
        "print(len(test_ids))\n",
        "print(output.shape)\n",
        "\n",
        "output = np.vstack((test_ids, output)).T\n",
        "\n",
        "# write to csv file\n",
        "with open('taska_submission.csv', 'w') as myFile:  \n",
        "    writer = csv.writer(myFile)\n",
        "    writer.writerows(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKnFqtgMIgTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "- re-run training with lstm conv\n",
        "- re-run submission generation\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BADMNp-BS9bW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}